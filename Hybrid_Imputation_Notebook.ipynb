{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Hybrid Correlation + PMM + MICE Imputer\n",
    "\n",
    "A sophisticated missing data imputation library that combines three powerful techniques:\n",
    "- **Correlation Analysis**: Identifies optimal predictor sets based on feature correlations\n",
    "- **PMM (Predictive Mean Matching)**: Preserves data distribution through semi-parametric imputation\n",
    "- **MICE (Multivariate Imputation by Chained Equations)**: Iteratively refines imputations for better accuracy\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Correlation Analysis Phase**: Computes pairwise correlations and identifies strongly correlated features\n",
    "2. **Predictive Mean Matching Phase**: Fits prediction models and selects from donor pools to preserve distribution\n",
    "3. **MICE Iteration Phase**: Iteratively imputes each variable using updated values from other variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas scikit-learn scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_cell"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "correlation_analyzer_header"
   },
   "source": [
    "## 1. Correlation Analyzer Module\n",
    "\n",
    "This module analyzes correlations between features to determine optimal imputation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "correlation_analyzer"
   },
   "outputs": [],
   "source": [
    "class CorrelationAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes correlations between features to determine optimal imputation strategies.\n",
    "    Uses correlation coefficients to identify the most predictive features for each\n",
    "    variable with missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, correlation_threshold: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize the correlation analyzer.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        correlation_threshold : float, default=0.3\n",
    "            Minimum absolute correlation coefficient to consider a feature\n",
    "            as a potential predictor\n",
    "        \"\"\"\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.correlation_matrix = None\n",
    "        self.predictor_sets = {}\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> 'CorrelationAnalyzer':\n",
    "        \"\"\"\n",
    "        Compute correlation matrix and identify predictor sets.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            The dataset to analyze\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : CorrelationAnalyzer\n",
    "            Fitted analyzer\n",
    "        \"\"\"\n",
    "        # Compute correlation matrix\n",
    "        self.correlation_matrix = data.corr(method='pearson')\n",
    "\n",
    "        # For each column, identify highly correlated predictors\n",
    "        for col in data.columns:\n",
    "            correlations = self.correlation_matrix[col].drop(col)\n",
    "            # Select features with correlation above threshold\n",
    "            strong_correlates = correlations[\n",
    "                abs(correlations) >= self.correlation_threshold\n",
    "            ].sort_values(key=abs, ascending=False)\n",
    "\n",
    "            self.predictor_sets[col] = list(strong_correlates.index)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_predictors(self, target_column: str, max_predictors: int = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get the list of best predictor columns for a target column.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_column : str\n",
    "            The column to find predictors for\n",
    "        max_predictors : int, optional\n",
    "            Maximum number of predictors to return\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        predictors : List[str]\n",
    "            List of predictor column names\n",
    "        \"\"\"\n",
    "        if target_column not in self.predictor_sets:\n",
    "            return []\n",
    "\n",
    "        predictors = self.predictor_sets[target_column]\n",
    "\n",
    "        if max_predictors is not None:\n",
    "            predictors = predictors[:max_predictors]\n",
    "\n",
    "        return predictors\n",
    "\n",
    "    def get_correlation_strength(self, col1: str, col2: str) -> float:\n",
    "        \"\"\"\n",
    "        Get the correlation coefficient between two columns.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        col1, col2 : str\n",
    "            Column names\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        correlation : float\n",
    "            Pearson correlation coefficient\n",
    "        \"\"\"\n",
    "        if self.correlation_matrix is None:\n",
    "            raise ValueError(\"Analyzer not fitted. Call fit() first.\")\n",
    "\n",
    "        return self.correlation_matrix.loc[col1, col2]\n",
    "\n",
    "    def get_imputation_order(self, columns_with_missing: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Determine optimal order for imputing columns based on correlations.\n",
    "        Columns with more complete predictors should be imputed first.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        columns_with_missing : List[str]\n",
    "            Columns that have missing values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        ordered_columns : List[str]\n",
    "            Columns ordered by imputation priority\n",
    "        \"\"\"\n",
    "        # Score each column by number of available predictors\n",
    "        scores = {}\n",
    "        for col in columns_with_missing:\n",
    "            predictors = self.get_predictors(col)\n",
    "            # Prefer columns that have many strong predictors\n",
    "            scores[col] = len(predictors)\n",
    "\n",
    "        # Sort by score (descending) - impute columns with fewer predictors first\n",
    "        # This allows later imputations to benefit from earlier ones\n",
    "        ordered = sorted(scores.items(), key=lambda x: x[1])\n",
    "\n",
    "        return [col for col, _ in ordered]\n",
    "\n",
    "    def visualize_correlations(self, figsize: Tuple[int, int] = (12, 10)):\n",
    "        \"\"\"\n",
    "        Create a heatmap visualization of the correlation matrix.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : Tuple[int, int]\n",
    "            Figure size for the plot\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(\n",
    "            self.correlation_matrix,\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt='.2f'\n",
    "        )\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmm_imputer_header"
   },
   "source": [
    "## 2. Predictive Mean Matching (PMM) Imputer Module\n",
    "\n",
    "Implements PMM algorithm for semi-parametric imputation that preserves data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmm_imputer"
   },
   "outputs": [],
   "source": [
    "class PMMImputer:\n",
    "    \"\"\"\n",
    "    Predictive Mean Matching (PMM) imputer.\n",
    "\n",
    "    PMM is a semi-parametric imputation method that:\n",
    "    1. Fits a prediction model on observed data\n",
    "    2. Predicts values for missing data\n",
    "    3. Finds observed values with similar predictions\n",
    "    4. Randomly selects from these \"donor\" values\n",
    "\n",
    "    This preserves the distribution of the original data better than\n",
    "    simple regression imputation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_neighbors: int = 5,\n",
    "        model_type: str = 'linear',\n",
    "        random_state: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the PMM imputer.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_neighbors : int, default=5\n",
    "            Number of nearest neighbors to consider for donor pool\n",
    "        model_type : str, default='linear'\n",
    "            Type of prediction model: 'linear', 'bayesian', or 'rf' (random forest)\n",
    "        random_state : int, optional\n",
    "            Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.model_type = model_type\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # Initialize the prediction model\n",
    "        if model_type == 'linear':\n",
    "            self.model = LinearRegression()\n",
    "        elif model_type == 'bayesian':\n",
    "            self.model = BayesianRidge()\n",
    "        elif model_type == 'rf':\n",
    "            self.model = RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def fit_transform(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        target_column: str,\n",
    "        predictor_columns: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Impute missing values in the target column using PMM.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            The dataset\n",
    "        target_column : str\n",
    "            Column to impute\n",
    "        predictor_columns : List[str]\n",
    "            Columns to use as predictors\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_values : np.ndarray\n",
    "            The imputed column (complete, no missing values)\n",
    "        \"\"\"\n",
    "        # Separate observed and missing data\n",
    "        target = data[target_column]\n",
    "        predictors = data[predictor_columns]\n",
    "\n",
    "        # Handle case where predictors might have missing values\n",
    "        # For now, we'll only use complete cases in predictors\n",
    "        complete_mask = ~predictors.isnull().any(axis=1)\n",
    "        observed_mask = ~target.isnull() & complete_mask\n",
    "        missing_mask = target.isnull() & complete_mask\n",
    "\n",
    "        if missing_mask.sum() == 0:\n",
    "            # No missing values to impute\n",
    "            return target.values\n",
    "\n",
    "        if observed_mask.sum() < self.n_neighbors:\n",
    "            # Not enough observed data for PMM, fall back to mean imputation\n",
    "            mean_value = target[observed_mask].mean()\n",
    "            result = target.copy()\n",
    "            result[missing_mask] = mean_value\n",
    "            return result.values\n",
    "\n",
    "        # Get observed and missing predictor matrices\n",
    "        X_observed = predictors[observed_mask]\n",
    "        X_missing = predictors[missing_mask]\n",
    "        y_observed = target[observed_mask].values\n",
    "\n",
    "        # Scale the predictors\n",
    "        X_observed_scaled = self.scaler.fit_transform(X_observed)\n",
    "        X_missing_scaled = self.scaler.transform(X_missing)\n",
    "\n",
    "        # Fit the prediction model\n",
    "        self.model.fit(X_observed_scaled, y_observed)\n",
    "\n",
    "        # Predict for both observed and missing\n",
    "        y_observed_pred = self.model.predict(X_observed_scaled)\n",
    "        y_missing_pred = self.model.predict(X_missing_scaled)\n",
    "\n",
    "        # For each missing value, find donors and select one\n",
    "        imputed_values = np.zeros(missing_mask.sum())\n",
    "\n",
    "        for i, pred_value in enumerate(y_missing_pred):\n",
    "            # Find the k nearest observed predictions\n",
    "            distances = np.abs(y_observed_pred - pred_value)\n",
    "            nearest_indices = np.argpartition(distances, min(self.n_neighbors, len(distances) - 1))[:self.n_neighbors]\n",
    "\n",
    "            # Randomly select one donor from the nearest neighbors\n",
    "            donor_idx = self.rng.choice(nearest_indices)\n",
    "            imputed_values[i] = y_observed[donor_idx]\n",
    "\n",
    "        # Create result array\n",
    "        result = target.copy()\n",
    "        result[missing_mask] = imputed_values\n",
    "\n",
    "        return result.values\n",
    "\n",
    "    def impute_column(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        target_column: str,\n",
    "        predictor_columns: Optional[List[str]] = None\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Convenience method to impute a single column and return as Series.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            The dataset\n",
    "        target_column : str\n",
    "            Column to impute\n",
    "        predictor_columns : List[str], optional\n",
    "            Columns to use as predictors. If None, uses all other columns.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_column : pd.Series\n",
    "            The imputed column\n",
    "        \"\"\"\n",
    "        if predictor_columns is None:\n",
    "            predictor_columns = [col for col in data.columns if col != target_column]\n",
    "\n",
    "        imputed_values = self.fit_transform(data, target_column, predictor_columns)\n",
    "        return pd.Series(imputed_values, index=data.index, name=target_column)\n",
    "\n",
    "\n",
    "class AdaptivePMMImputer(PMMImputer):\n",
    "    \"\"\"\n",
    "    Adaptive PMM imputer that adjusts n_neighbors based on data availability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_neighbors: int = 5,\n",
    "        min_neighbors: int = 3,\n",
    "        model_type: str = 'linear',\n",
    "        random_state: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize adaptive PMM imputer.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_neighbors : int, default=5\n",
    "            Target number of nearest neighbors\n",
    "        min_neighbors : int, default=3\n",
    "            Minimum number of neighbors to use\n",
    "        model_type : str, default='linear'\n",
    "            Type of prediction model\n",
    "        random_state : int, optional\n",
    "            Random state for reproducibility\n",
    "        \"\"\"\n",
    "        super().__init__(n_neighbors, model_type, random_state)\n",
    "        self.min_neighbors = min_neighbors\n",
    "\n",
    "    def fit_transform(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        target_column: str,\n",
    "        predictor_columns: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Impute with adaptive neighbor selection.\n",
    "        \"\"\"\n",
    "        target = data[target_column]\n",
    "        predictors = data[predictor_columns]\n",
    "\n",
    "        complete_mask = ~predictors.isnull().any(axis=1)\n",
    "        observed_mask = ~target.isnull() & complete_mask\n",
    "        n_observed = observed_mask.sum()\n",
    "\n",
    "        # Adapt n_neighbors based on available data\n",
    "        if n_observed < self.min_neighbors:\n",
    "            # Fall back to mean imputation\n",
    "            mean_value = target[observed_mask].mean()\n",
    "            result = target.copy()\n",
    "            result[target.isnull()] = mean_value\n",
    "            return result.values\n",
    "\n",
    "        # Adjust n_neighbors to available data\n",
    "        original_n = self.n_neighbors\n",
    "        self.n_neighbors = min(self.n_neighbors, max(self.min_neighbors, n_observed // 3))\n",
    "\n",
    "        # Call parent fit_transform\n",
    "        result = super().fit_transform(data, target_column, predictor_columns)\n",
    "\n",
    "        # Restore original n_neighbors\n",
    "        self.n_neighbors = original_n\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hybrid_imputer_header"
   },
   "source": [
    "## 3. Hybrid MICE Imputer Module\n",
    "\n",
    "Combines Correlation Analysis + PMM + MICE for advanced missing data imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hybrid_imputer"
   },
   "outputs": [],
   "source": "class HybridMICEImputer:\n    \"\"\"\n    Hybrid imputation model combining:\n    - Correlation Analysis: To identify optimal predictor sets\n    - PMM (Predictive Mean Matching): For distribution-preserving imputation\n    - MICE (Multivariate Imputation by Chained Equations): For iterative refinement\n\n    This hybrid approach:\n    1. Uses correlation analysis to determine which features best predict each missing variable\n    2. Employs PMM to impute values while preserving data distribution\n    3. Iterates using MICE framework to refine imputations based on newly imputed values\n    \"\"\"\n\n    def __init__(\n        self,\n        n_iterations: int = 10,\n        n_neighbors: int = 5,\n        correlation_threshold: float = 0.3,\n        max_predictors: int = 10,\n        pmm_model_type: str = 'linear',\n        convergence_threshold: float = 0.001,\n        random_state: Optional[int] = None,\n        verbose: bool = False,\n        exclude_columns: Optional[List[str]] = None\n    ):\n        \"\"\"\n        Initialize the Hybrid MICE imputer.\n\n        Parameters:\n        -----------\n        n_iterations : int, default=10\n            Maximum number of MICE iterations\n        n_neighbors : int, default=5\n            Number of neighbors for PMM\n        correlation_threshold : float, default=0.3\n            Minimum correlation to consider for predictor selection\n        max_predictors : int, default=10\n            Maximum number of predictors to use per variable\n        pmm_model_type : str, default='linear'\n            Model type for PMM: 'linear', 'bayesian', or 'rf'\n        convergence_threshold : float, default=0.001\n            Threshold for convergence detection\n        random_state : int, optional\n            Random state for reproducibility\n        verbose : bool, default=False\n            Whether to print progress information\n        exclude_columns : List[str], optional\n            Columns to exclude from imputation (e.g., ID columns like 'ptid')\n        \"\"\"\n        self.n_iterations = n_iterations\n        self.n_neighbors = n_neighbors\n        self.correlation_threshold = correlation_threshold\n        self.max_predictors = max_predictors\n        self.pmm_model_type = pmm_model_type\n        self.convergence_threshold = convergence_threshold\n        self.random_state = random_state\n        self.verbose = verbose\n        self.exclude_columns = exclude_columns or []\n\n        # Components\n        self.correlation_analyzer = CorrelationAnalyzer(\n            correlation_threshold=correlation_threshold\n        )\n        self.imputer = AdaptivePMMImputer(\n            n_neighbors=n_neighbors,\n            model_type=pmm_model_type,\n            random_state=random_state\n        )\n\n        # State\n        self.missing_indicators = None\n        self.columns_with_missing = []\n        self.numeric_columns_with_missing = []\n        self.imputation_order = []\n        self.convergence_history = []\n\n    def _identify_missing_data(self, data: pd.DataFrame) -> None:\n        \"\"\"Identify columns with missing data and create missing indicators.\"\"\"\n        self.missing_indicators = data.isnull()\n        self.columns_with_missing = [\n            col for col in data.columns if self.missing_indicators[col].any()\n        ]\n\n        # Filter to only numeric columns for PMM imputation\n        # Exclude non-numeric columns and any user-specified exclusions\n        self.numeric_columns_with_missing = [\n            col for col in self.columns_with_missing\n            if col not in self.exclude_columns\n            and data[col].dtype in [np.float64, np.float32, np.int64, np.int32, np.float16, np.int16, np.int8]\n        ]\n\n        if self.verbose:\n            print(f\"Columns with missing data: {len(self.columns_with_missing)}\")\n            print(f\"Numeric columns to impute with PMM: {len(self.numeric_columns_with_missing)}\")\n            if len(self.columns_with_missing) > len(self.numeric_columns_with_missing):\n                excluded = set(self.columns_with_missing) - set(self.numeric_columns_with_missing)\n                print(f\"Excluded columns (non-numeric or specified): {list(excluded)}\")\n            print()\n            for col in self.columns_with_missing:\n                n_missing = self.missing_indicators[col].sum()\n                pct_missing = 100 * n_missing / len(data)\n                col_type = \"numeric (PMM)\" if col in self.numeric_columns_with_missing else \"excluded\"\n                print(f\"  {col}: {n_missing} ({pct_missing:.2f}%) - {col_type}\")\n\n    def _initialize_imputation(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Initialize missing values with simple mean imputation.\n        This provides a starting point for the MICE iterations.\n        \"\"\"\n        imputed = data.copy()\n\n        for col in self.columns_with_missing:\n            if imputed[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n                # Numerical: use mean\n                mean_val = imputed[col].mean()\n                imputed[col].fillna(mean_val, inplace=True)\n            else:\n                # Categorical: use mode\n                mode_val = imputed[col].mode()[0] if len(imputed[col].mode()) > 0 else imputed[col].iloc[0]\n                imputed[col].fillna(mode_val, inplace=True)\n\n        return imputed\n\n    def _compute_convergence_metric(\n        self,\n        data_current: pd.DataFrame,\n        data_previous: pd.DataFrame\n    ) -> float:\n        \"\"\"\n        Compute convergence metric based on change in imputed values.\n        Only considers numeric columns that are being imputed.\n        \"\"\"\n        if data_previous is None:\n            return float('inf')\n\n        total_change = 0\n        n_imputed = 0\n\n        # Only compute convergence for numeric columns being imputed\n        for col in self.numeric_columns_with_missing:\n            missing_mask = self.missing_indicators[col]\n            if missing_mask.any():\n                current_vals = data_current.loc[missing_mask, col]\n                previous_vals = data_previous.loc[missing_mask, col]\n\n                # Normalize by standard deviation\n                std = data_current[col].std()\n                if std > 0:\n                    change = np.mean(np.abs(current_vals - previous_vals) / std)\n                else:\n                    change = np.mean(np.abs(current_vals - previous_vals))\n\n                total_change += change\n                n_imputed += 1\n\n        return total_change / n_imputed if n_imputed > 0 else 0\n\n    def fit_transform(\n        self,\n        data: pd.DataFrame,\n        columns_to_impute: Optional[List[str]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Impute missing values using the hybrid Correlation + PMM + MICE approach.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            Dataset with missing values\n        columns_to_impute : List[str], optional\n            Specific columns to impute. If None, imputes all columns with missing values.\n\n        Returns:\n        --------\n        imputed_data : pd.DataFrame\n            Dataset with imputed values\n        \"\"\"\n        if data.isnull().sum().sum() == 0:\n            if self.verbose:\n                print(\"No missing data found. Returning original data.\")\n            return data.copy()\n\n        # Identify missing data\n        self._identify_missing_data(data)\n\n        # Filter to requested columns\n        if columns_to_impute is not None:\n            self.columns_with_missing = [\n                col for col in self.columns_with_missing if col in columns_to_impute\n            ]\n            self.numeric_columns_with_missing = [\n                col for col in self.numeric_columns_with_missing if col in columns_to_impute\n            ]\n\n        if not self.numeric_columns_with_missing:\n            if self.verbose:\n                print(\"No numeric columns to impute with PMM. Returning data with simple imputation for non-numeric columns.\")\n            # Still do simple imputation for non-numeric columns\n            return self._initialize_imputation(data)\n\n        # Initialize with simple imputation\n        imputed = self._initialize_imputation(data)\n\n        # Fit correlation analyzer on complete cases first, but only on numeric columns\n        numeric_cols = [col for col in data.columns\n                       if data[col].dtype in [np.float64, np.float32, np.int64, np.int32, np.float16, np.int16, np.int8]]\n        complete_data = data[numeric_cols].dropna()\n        if len(complete_data) > 0:\n            self.correlation_analyzer.fit(complete_data)\n        else:\n            # If no complete cases, use initialized numeric data\n            self.correlation_analyzer.fit(imputed[numeric_cols])\n\n        # Determine imputation order based on correlations (only for numeric columns)\n        self.imputation_order = self.correlation_analyzer.get_imputation_order(\n            self.numeric_columns_with_missing\n        )\n\n        if self.verbose:\n            print(f\"Imputation order: {self.imputation_order}\")\n            print(f\"Starting MICE iterations (max {self.n_iterations})...\")\n\n        # MICE iterations\n        previous_imputed = None\n        self.convergence_history = []\n\n        for iteration in range(self.n_iterations):\n            if self.verbose:\n                print(f\"\\nIteration {iteration + 1}/{self.n_iterations}\")\n\n            # Iterate through each column with missing values\n            for col in self.imputation_order:\n                # Get correlation-based predictors\n                predictors = self.correlation_analyzer.get_predictors(\n                    col,\n                    max_predictors=self.max_predictors\n                )\n\n                # If no predictors found, use all other columns\n                if not predictors:\n                    predictors = [c for c in imputed.columns if c != col]\n\n                # Filter out predictors that are all NaN or have too many NaNs\n                valid_predictors = []\n                for pred in predictors:\n                    if imputed[pred].isnull().sum() < len(imputed) * 0.5:\n                        valid_predictors.append(pred)\n\n                if not valid_predictors:\n                    if self.verbose:\n                        print(f\"  {col}: No valid predictors, skipping\")\n                    continue\n\n                # Apply PMM imputation\n                try:\n                    # Create temporary dataset with only needed columns\n                    temp_data = imputed[[col] + valid_predictors].copy()\n\n                    # CRITICAL FIX: Restore original missing values in the target column\n                    # so that PMM can actually see them and perform proper imputation\n                    missing_mask = self.missing_indicators[col]\n                    temp_data.loc[missing_mask, col] = np.nan\n\n                    # Impute the column using PMM\n                    imputed_col = self.imputer.fit_transform(\n                        temp_data,\n                        col,\n                        valid_predictors\n                    )\n\n                    # Update only the originally missing values\n                    imputed.loc[missing_mask, col] = imputed_col[missing_mask]\n\n                    if self.verbose:\n                        print(f\"  {col}: Imputed with {len(valid_predictors)} predictors\")\n\n                except Exception as e:\n                    if self.verbose:\n                        print(f\"  {col}: Error during imputation - {str(e)}\")\n                    continue\n\n            # Check convergence\n            convergence = self._compute_convergence_metric(imputed, previous_imputed)\n            self.convergence_history.append(convergence)\n\n            if self.verbose:\n                print(f\"Convergence metric: {convergence:.6f}\")\n\n            if convergence < self.convergence_threshold:\n                if self.verbose:\n                    print(f\"Converged at iteration {iteration + 1}\")\n                break\n\n            previous_imputed = imputed.copy()\n\n        if self.verbose:\n            print(\"\\nImputation complete!\")\n            print(f\"Total iterations: {len(self.convergence_history)}\")\n\n        return imputed\n\n    def fit(self, data: pd.DataFrame) -> 'HybridMICEImputer':\n        \"\"\"\n        Fit the imputer (mainly for correlation analysis).\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            Training data\n\n        Returns:\n        --------\n        self : HybridMICEImputer\n        \"\"\"\n        # Only fit on numeric columns\n        numeric_cols = [col for col in data.columns\n                       if data[col].dtype in [np.float64, np.float32, np.int64, np.int32, np.float16, np.int16, np.int8]]\n        complete_data = data[numeric_cols].dropna()\n        if len(complete_data) > 0:\n            self.correlation_analyzer.fit(complete_data)\n        else:\n            # If no complete cases, use all numeric data\n            self.correlation_analyzer.fit(data[numeric_cols])\n\n        return self\n\n    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform data using fitted imputer.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            Data to impute\n\n        Returns:\n        --------\n        imputed_data : pd.DataFrame\n        \"\"\"\n        return self.fit_transform(data)\n\n    def get_diagnostics(self) -> Dict[str, Union[List, pd.DataFrame]]:\n        \"\"\"\n        Get diagnostic information about the imputation process.\n\n        Returns:\n        --------\n        diagnostics : dict\n            Dictionary containing diagnostic information\n        \"\"\"\n        excluded_columns = list(set(self.columns_with_missing) - set(self.numeric_columns_with_missing))\n\n        diagnostics = {\n            'convergence_history': self.convergence_history,\n            'imputation_order': self.imputation_order,\n            'columns_with_missing': self.columns_with_missing,\n            'numeric_columns_imputed': self.numeric_columns_with_missing,\n            'excluded_columns': excluded_columns,\n            'correlation_matrix': self.correlation_analyzer.correlation_matrix,\n            'predictor_sets': self.correlation_analyzer.predictor_sets\n        }\n\n        return diagnostics\n\n    def plot_convergence(self):\n        \"\"\"\n        Plot the convergence history.\n        \"\"\"\n        if not self.convergence_history:\n            print(\"No convergence history available. Run fit_transform first.\")\n            return\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(range(1, len(self.convergence_history) + 1), self.convergence_history, marker='o')\n        plt.axhline(y=self.convergence_threshold, color='r', linestyle='--', label='Convergence threshold')\n        plt.xlabel('Iteration')\n        plt.ylabel('Convergence Metric')\n        plt.title('MICE Convergence History')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper_functions_header"
   },
   "source": [
    "## 4. Helper Functions\n",
    "\n",
    "Function to create sample data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper_functions"
   },
   "outputs": [],
   "source": [
    "def create_sample_data_with_missing(n_samples=1000, missing_rate=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a sample dataset with missing values for demonstration.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    missing_rate : float\n",
    "        Proportion of values to set as missing\n",
    "    random_state : int\n",
    "        Random seed\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data : pd.DataFrame\n",
    "        Dataset with missing values\n",
    "    data_complete : pd.DataFrame\n",
    "        Original complete dataset (for comparison)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Generate correlated features\n",
    "    # Feature 1: base random variable\n",
    "    x1 = np.random.randn(n_samples)\n",
    "\n",
    "    # Feature 2: strongly correlated with x1\n",
    "    x2 = 0.8 * x1 + 0.2 * np.random.randn(n_samples)\n",
    "\n",
    "    # Feature 3: moderately correlated with x1 and x2\n",
    "    x3 = 0.5 * x1 + 0.3 * x2 + 0.4 * np.random.randn(n_samples)\n",
    "\n",
    "    # Feature 4: weakly correlated\n",
    "    x4 = 0.2 * x1 + 0.8 * np.random.randn(n_samples)\n",
    "\n",
    "    # Feature 5: independent\n",
    "    x5 = np.random.randn(n_samples)\n",
    "\n",
    "    # Create DataFrame\n",
    "    data_complete = pd.DataFrame({\n",
    "        'feature1': x1,\n",
    "        'feature2': x2,\n",
    "        'feature3': x3,\n",
    "        'feature4': x4,\n",
    "        'feature5': x5\n",
    "    })\n",
    "\n",
    "    # Add a target variable\n",
    "    data_complete['target'] = (\n",
    "        2 * x1 + 1.5 * x2 - 0.5 * x3 + np.random.randn(n_samples) * 0.5\n",
    "    )\n",
    "\n",
    "    # Create missing values\n",
    "    data_with_missing = data_complete.copy()\n",
    "\n",
    "    for col in data_with_missing.columns:\n",
    "        # Randomly select indices to set as missing\n",
    "        n_missing = int(n_samples * missing_rate)\n",
    "        missing_indices = np.random.choice(n_samples, n_missing, replace=False)\n",
    "        data_with_missing.loc[missing_indices, col] = np.nan\n",
    "\n",
    "    return data_with_missing, data_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examples_header"
   },
   "source": [
    "---\n",
    "\n",
    "# Examples and Usage\n",
    "\n",
    "Let's explore different use cases of the Hybrid MICE Imputer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example1_header"
   },
   "source": [
    "## Example 1: Basic Usage\n",
    "\n",
    "Demonstrate basic imputation with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example1"
   },
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data_missing, data_complete = create_sample_data_with_missing(\n",
    "    n_samples=500,\n",
    "    missing_rate=0.15\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Example 1: Basic Usage\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nOriginal data with missing values:\")\n",
    "print(data_missing.head(10))\n",
    "print(\"\\nMissing value statistics:\")\n",
    "print(data_missing.isnull().sum())\n",
    "\n",
    "# Initialize and fit the hybrid imputer\n",
    "imputer = HybridMICEImputer(\n",
    "    n_iterations=10,\n",
    "    n_neighbors=5,\n",
    "    correlation_threshold=0.3,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Impute missing values\n",
    "data_imputed = imputer.fit_transform(data_missing)\n",
    "\n",
    "print(\"\\n\\nImputed data:\")\n",
    "print(data_imputed.head(10))\n",
    "\n",
    "# Calculate imputation accuracy (RMSE)\n",
    "print(\"\\n\\nImputation Quality Assessment:\")\n",
    "for col in data_missing.columns:\n",
    "    missing_mask = data_missing[col].isnull()\n",
    "    if missing_mask.any():\n",
    "        true_values = data_complete.loc[missing_mask, col]\n",
    "        imputed_values = data_imputed.loc[missing_mask, col]\n",
    "        rmse = np.sqrt(np.mean((true_values - imputed_values) ** 2))\n",
    "        print(f\"{col}: RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example2_header"
   },
   "source": [
    "## Example 2: Comparing Different Model Types\n",
    "\n",
    "Compare performance of different PMM model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example2"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 2: Comparing Model Types\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create data with more missing values\n",
    "data_missing, data_complete = create_sample_data_with_missing(\n",
    "    n_samples=300,\n",
    "    missing_rate=0.30\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(data_missing)} samples\")\n",
    "print(f\"Missing rate: ~30%\")\n",
    "\n",
    "# Try different model configurations\n",
    "configs = [\n",
    "    {'name': 'Linear PMM', 'pmm_model_type': 'linear'},\n",
    "    {'name': 'Bayesian PMM', 'pmm_model_type': 'bayesian'},\n",
    "    {'name': 'Random Forest PMM', 'pmm_model_type': 'rf'}\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\n\\nTesting: {config['name']}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    imputer = HybridMICEImputer(\n",
    "        n_iterations=15,\n",
    "        n_neighbors=7,\n",
    "        pmm_model_type=config['pmm_model_type'],\n",
    "        correlation_threshold=0.25,\n",
    "        verbose=False,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    data_imputed = imputer.fit_transform(data_missing)\n",
    "\n",
    "    # Calculate overall RMSE\n",
    "    total_rmse = 0\n",
    "    n_cols = 0\n",
    "\n",
    "    for col in data_missing.columns:\n",
    "        missing_mask = data_missing[col].isnull()\n",
    "        if missing_mask.any():\n",
    "            true_values = data_complete.loc[missing_mask, col]\n",
    "            imputed_values = data_imputed.loc[missing_mask, col]\n",
    "            rmse = np.sqrt(np.mean((true_values - imputed_values) ** 2))\n",
    "            total_rmse += rmse\n",
    "            n_cols += 1\n",
    "\n",
    "    avg_rmse = total_rmse / n_cols if n_cols > 0 else 0\n",
    "    results[config['name']] = avg_rmse\n",
    "\n",
    "    print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "    print(f\"Iterations to convergence: {len(imputer.convergence_history)}\")\n",
    "\n",
    "print(\"\\n\\nComparison Summary:\")\n",
    "print(\"-\"*40)\n",
    "for name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
    "    print(f\"{name}: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example3_header"
   },
   "source": [
    "## Example 3: Diagnostics and Visualization\n",
    "\n",
    "Explore diagnostic features and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example3"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 3: Diagnostics and Visualization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sample data\n",
    "data_missing, _ = create_sample_data_with_missing(\n",
    "    n_samples=400,\n",
    "    missing_rate=0.20\n",
    ")\n",
    "\n",
    "# Initialize imputer\n",
    "imputer = HybridMICEImputer(\n",
    "    n_iterations=20,\n",
    "    verbose=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Impute\n",
    "data_imputed = imputer.fit_transform(data_missing)\n",
    "\n",
    "# Get diagnostics\n",
    "diagnostics = imputer.get_diagnostics()\n",
    "\n",
    "print(\"\\nImputation Order (based on correlations):\")\n",
    "for i, col in enumerate(diagnostics['imputation_order'], 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "print(\"\\n\\nPredictor Sets (correlation-based):\")\n",
    "for col, predictors in diagnostics['predictor_sets'].items():\n",
    "    if col in diagnostics['columns_with_missing']:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Top predictors: {predictors[:3]}\")\n",
    "\n",
    "print(\"\\n\\nCorrelation Matrix:\")\n",
    "print(diagnostics['correlation_matrix'])\n",
    "\n",
    "print(\"\\n\\nConvergence History:\")\n",
    "for i, conv in enumerate(diagnostics['convergence_history'], 1):\n",
    "    print(f\"Iteration {i}: {conv:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plot_convergence_header"
   },
   "source": [
    "### Visualize Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_convergence"
   },
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "imputer.plot_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plot_correlations_header"
   },
   "source": [
    "### Visualize Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_correlations"
   },
   "outputs": [],
   "source": [
    "# Visualize correlation matrix\n",
    "imputer.correlation_analyzer.visualize_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example4_header"
   },
   "source": [
    "## Example 4: Partial Imputation (Specific Columns)\n",
    "\n",
    "Demonstrate imputing only specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example4"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 4: Partial Imputation (Specific Columns)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sample data\n",
    "data_missing, data_complete = create_sample_data_with_missing(\n",
    "    n_samples=500,\n",
    "    missing_rate=0.15\n",
    ")\n",
    "\n",
    "print(\"\\nImputing only 'feature1' and 'target' columns...\")\n",
    "\n",
    "# Initialize imputer\n",
    "imputer = HybridMICEImputer(\n",
    "    n_iterations=10,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Impute only specific columns\n",
    "data_imputed = imputer.fit_transform(\n",
    "    data_missing,\n",
    "    columns_to_impute=['feature1', 'target']\n",
    ")\n",
    "\n",
    "# Check which columns were imputed\n",
    "print(\"\\n\\nMissing values after imputation:\")\n",
    "print(data_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_data_header"
   },
   "source": [
    "## Example 5: Using Your Own Data\n",
    "\n",
    "Template for using the imputer with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_data"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 5: Using Your Own Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Uncomment and modify this section to use your own data:\n",
    "\n",
    "# # Load your data\n",
    "# your_data = pd.read_csv('your_file.csv')\n",
    "#\n",
    "# # Or upload a file in Google Colab:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# your_data = pd.read_csv(list(uploaded.keys())[0])\n",
    "#\n",
    "# # Initialize imputer with custom parameters\n",
    "# imputer = HybridMICEImputer(\n",
    "#     n_iterations=15,\n",
    "#     n_neighbors=5,\n",
    "#     correlation_threshold=0.3,\n",
    "#     pmm_model_type='linear',  # Options: 'linear', 'bayesian', 'rf'\n",
    "#     verbose=True,\n",
    "#     random_state=42\n",
    "# )\n",
    "#\n",
    "# # Impute missing values\n",
    "# your_data_imputed = imputer.fit_transform(your_data)\n",
    "#\n",
    "# # View results\n",
    "# print(\"\\nOriginal data:\")\n",
    "# print(your_data.head())\n",
    "# print(\"\\nMissing values:\", your_data.isnull().sum().sum())\n",
    "#\n",
    "# print(\"\\nImputed data:\")\n",
    "# print(your_data_imputed.head())\n",
    "# print(\"\\nMissing values:\", your_data_imputed.isnull().sum().sum())\n",
    "#\n",
    "# # Plot convergence\n",
    "# imputer.plot_convergence()\n",
    "#\n",
    "# # Download the imputed data\n",
    "# your_data_imputed.to_csv('imputed_data.csv', index=False)\n",
    "# files.download('imputed_data.csv')\n",
    "\n",
    "print(\"\\nUncomment and modify the code above to use your own data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Example 6: Handling Non-Numeric and ID Columns\n\nDemonstrates automatic exclusion of non-numeric columns and manual exclusion of ID columns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"Example 6: Handling Non-Numeric and ID Columns\")\nprint(\"=\"*70)\n\n# Create sample data with mixed types (like a real dataset)\nnp.random.seed(42)\nn_samples = 200\n\n# Create numeric features\nfeature1 = np.random.randn(n_samples)\nfeature2 = 0.7 * feature1 + 0.3 * np.random.randn(n_samples)\nage = np.random.randint(20, 80, n_samples).astype(float)\nscore = 50 + 10 * feature1 + np.random.randn(n_samples) * 5\n\n# Create non-numeric columns\nptid = [f\"PT{i:04d}\" for i in range(n_samples)]\ndiagnosis = np.random.choice(['Control', 'AD', 'MCI'], n_samples)\ngender = np.random.choice(['M', 'F'], n_samples)\n\n# Create DataFrame with mixed types\ndata_mixed = pd.DataFrame({\n    'ptid': ptid,\n    'diagnosis': diagnosis,\n    'gender': gender,\n    'age': age,\n    'feature1': feature1,\n    'feature2': feature2,\n    'score': score\n})\n\n# Introduce missing values in various columns\nmissing_rate = 0.20\nfor col in ['age', 'feature1', 'feature2', 'score', 'diagnosis']:\n    n_missing = int(n_samples * missing_rate)\n    missing_indices = np.random.choice(n_samples, n_missing, replace=False)\n    data_mixed.loc[missing_indices, col] = np.nan\n\nprint(\"\\nOriginal data with mixed types:\")\nprint(data_mixed.head(10))\nprint(\"\\nData types:\")\nprint(data_mixed.dtypes)\nprint(\"\\nMissing values:\")\nprint(data_mixed.isnull().sum())\n\n# Initialize imputer - it will automatically exclude non-numeric columns\n# and we can manually exclude 'ptid' as well\nimputer = HybridMICEImputer(\n    n_iterations=10,\n    n_neighbors=5,\n    correlation_threshold=0.3,\n    exclude_columns=['ptid'],  # Manually exclude patient ID\n    verbose=True,\n    random_state=42\n)\n\n# Impute missing values\ndata_imputed = imputer.fit_transform(data_mixed)\n\nprint(\"\\n\\nImputed data:\")\nprint(data_imputed.head(10))\nprint(\"\\nMissing values after imputation:\")\nprint(data_imputed.isnull().sum())\n\n# Get diagnostics to see what was excluded\ndiagnostics = imputer.get_diagnostics()\nprint(\"\\n\\nDiagnostic Information:\")\nprint(f\"Total columns with missing data: {len(diagnostics['columns_with_missing'])}\")\nprint(f\"Numeric columns imputed with PMM: {diagnostics['numeric_columns_imputed']}\")\nprint(f\"Excluded columns: {diagnostics['excluded_columns']}\")\nprint(\"\\nNote: Non-numeric columns (diagnosis, gender) and manually excluded columns (ptid)\")\nprint(\"      retain their original missing values or get simple mode imputation.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates the **Hybrid Correlation + PMM + MICE Imputer**, which combines:\n",
    "\n",
    "1. **Correlation Analysis** - Smart predictor selection based on feature correlations\n",
    "2. **Predictive Mean Matching** - Distribution-preserving imputation\n",
    "3. **MICE Framework** - Iterative refinement for improved accuracy\n",
    "\n",
    "### Key Features:\n",
    "- Multiple model types (Linear, Bayesian, Random Forest)\n",
    "- Automatic predictor selection\n",
    "- Convergence monitoring\n",
    "- Comprehensive diagnostics\n",
    "- Partial imputation support\n",
    "\n",
    "### When to Use:\n",
    "- Datasets with complex missing data patterns\n",
    "- When preserving data distribution is important\n",
    "- When features have strong correlations\n",
    "- For both research and production use cases\n",
    "\n",
    "---\n",
    "\n",
    "**Repository**: [https://github.com/CodeSakshamY/correlation-PMM-MICE](https://github.com/CodeSakshamY/correlation-PMM-MICE)\n",
    "\n",
    "**License**: MIT\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hybrid_Imputation_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}