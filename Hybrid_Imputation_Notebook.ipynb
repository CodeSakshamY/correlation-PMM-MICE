{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Hybrid Correlation + PMM + MICE Imputer\n",
    "\n",
    "A sophisticated missing data imputation library that combines three powerful techniques:\n",
    "- **Correlation Analysis**: Identifies optimal predictor sets based on feature correlations\n",
    "- **PMM (Predictive Mean Matching)**: Preserves data distribution through semi-parametric imputation\n",
    "- **MICE (Multivariate Imputation by Chained Equations)**: Iteratively refines imputations for better accuracy\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Correlation Analysis Phase**: Computes pairwise correlations and identifies strongly correlated features\n",
    "2. **Predictive Mean Matching Phase**: Fits prediction models and selects from donor pools to preserve distribution\n",
    "3. **MICE Iteration Phase**: Iteratively imputes each variable using updated values from other variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas scikit-learn scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_cell"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "correlation_analyzer_header"
   },
   "source": [
    "## 1. Correlation Analyzer Module\n",
    "\n",
    "This module analyzes correlations between features to determine optimal imputation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "correlation_analyzer"
   },
   "outputs": [],
   "source": "class CorrelationAnalyzer:\n    \"\"\"\n    Analyzes correlations between features to determine optimal imputation strategies.\n    Uses multiple correlation methods (Pearson, Spearman, Kendall) with weighted scoring\n    to identify the most predictive features for each variable with missing values.\n    \"\"\"\n\n    def __init__(self, correlation_threshold: float = 0.3, use_mixed_correlations: bool = True):\n        \"\"\"\n        Initialize the correlation analyzer.\n\n        Parameters:\n        -----------\n        correlation_threshold : float, default=0.3\n            Minimum absolute correlation coefficient to consider a feature\n            as a potential predictor\n        use_mixed_correlations : bool, default=True\n            If True, combines Pearson, Spearman, and Kendall correlations\n            for more robust correlation estimation\n        \"\"\"\n        self.correlation_threshold = correlation_threshold\n        self.use_mixed_correlations = use_mixed_correlations\n        self.correlation_matrix = None\n        self.pearson_matrix = None\n        self.spearman_matrix = None\n        self.kendall_matrix = None\n        self.combined_matrix = None\n        self.predictor_sets = {}\n        self.correlation_weights = {}\n\n    def fit(self, data: pd.DataFrame) -> 'CorrelationAnalyzer':\n        \"\"\"\n        Compute correlation matrix and identify predictor sets.\n        Uses multiple correlation methods for robust estimation.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            The dataset to analyze\n\n        Returns:\n        --------\n        self : CorrelationAnalyzer\n            Fitted analyzer\n        \"\"\"\n        if self.use_mixed_correlations:\n            # Compute multiple correlation matrices\n            self.pearson_matrix = data.corr(method='pearson')\n            self.spearman_matrix = data.corr(method='spearman')\n\n            # Kendall can be slow for large datasets, so we compute it selectively\n            try:\n                self.kendall_matrix = data.corr(method='kendall')\n            except:\n                self.kendall_matrix = None\n\n            # Combine correlations with weighted average\n            # Pearson: 50%, Spearman: 30%, Kendall: 20%\n            if self.kendall_matrix is not None:\n                self.combined_matrix = (\n                    0.5 * self.pearson_matrix.abs() +\n                    0.3 * self.spearman_matrix.abs() +\n                    0.2 * self.kendall_matrix.abs()\n                )\n            else:\n                # Without Kendall: Pearson 60%, Spearman 40%\n                self.combined_matrix = (\n                    0.6 * self.pearson_matrix.abs() +\n                    0.4 * self.spearman_matrix.abs()\n                )\n\n            # Use Pearson for the main correlation matrix (for signed correlations)\n            self.correlation_matrix = self.pearson_matrix\n        else:\n            # Simple Pearson correlation\n            self.correlation_matrix = data.corr(method='pearson')\n            self.combined_matrix = self.correlation_matrix.abs()\n\n        # For each column, identify highly correlated predictors\n        for col in data.columns:\n            if self.use_mixed_correlations:\n                # Use combined matrix for selection\n                correlations_abs = self.combined_matrix[col].drop(col)\n                correlations_signed = self.correlation_matrix[col].drop(col)\n            else:\n                correlations_abs = self.correlation_matrix[col].abs().drop(col)\n                correlations_signed = self.correlation_matrix[col].drop(col)\n\n            # Select features with correlation above threshold\n            strong_mask = correlations_abs >= self.correlation_threshold\n            strong_correlates = correlations_abs[strong_mask].sort_values(ascending=False)\n\n            # Store predictor list\n            self.predictor_sets[col] = list(strong_correlates.index)\n\n            # Store correlation weights for each predictor (normalized)\n            if len(strong_correlates) > 0:\n                # Use signed correlations for weights\n                weights = {}\n                for pred in strong_correlates.index:\n                    # Weight is the combined correlation strength\n                    weights[pred] = strong_correlates[pred]\n\n                # Normalize weights to sum to 1\n                total_weight = sum(weights.values())\n                if total_weight > 0:\n                    weights = {k: v / total_weight for k, v in weights.items()}\n\n                self.correlation_weights[col] = weights\n            else:\n                self.correlation_weights[col] = {}\n\n        return self\n\n    def get_predictors(self, target_column: str, max_predictors: int = None) -> List[str]:\n        \"\"\"\n        Get the list of best predictor columns for a target column.\n\n        Parameters:\n        -----------\n        target_column : str\n            The column to find predictors for\n        max_predictors : int, optional\n            Maximum number of predictors to return\n\n        Returns:\n        --------\n        predictors : List[str]\n            List of predictor column names\n        \"\"\"\n        if target_column not in self.predictor_sets:\n            return []\n\n        predictors = self.predictor_sets[target_column]\n\n        if max_predictors is not None:\n            predictors = predictors[:max_predictors]\n\n        return predictors\n\n    def get_correlation_strength(self, col1: str, col2: str) -> float:\n        \"\"\"\n        Get the correlation coefficient between two columns.\n\n        Parameters:\n        -----------\n        col1, col2 : str\n            Column names\n\n        Returns:\n        --------\n        correlation : float\n            Pearson correlation coefficient\n        \"\"\"\n        if self.correlation_matrix is None:\n            raise ValueError(\"Analyzer not fitted. Call fit() first.\")\n\n        return self.correlation_matrix.loc[col1, col2]\n\n    def get_predictor_weights(self, target_column: str) -> Dict[str, float]:\n        \"\"\"\n        Get normalized correlation weights for predictors of a target column.\n\n        Parameters:\n        -----------\n        target_column : str\n            The column to get predictor weights for\n\n        Returns:\n        --------\n        weights : Dict[str, float]\n            Dictionary mapping predictor names to their normalized weights\n        \"\"\"\n        if target_column not in self.correlation_weights:\n            return {}\n        return self.correlation_weights[target_column]\n\n    def get_imputation_order(self, columns_with_missing: List[str]) -> List[str]:\n        \"\"\"\n        Determine optimal order for imputing columns based on correlations.\n        Uses weighted scoring based on both quantity and quality of predictors.\n\n        Parameters:\n        -----------\n        columns_with_missing : List[str]\n            Columns that have missing values\n\n        Returns:\n        --------\n        ordered_columns : List[str]\n            Columns ordered by imputation priority\n        \"\"\"\n        # Score each column by quality and quantity of available predictors\n        scores = {}\n        for col in columns_with_missing:\n            predictors = self.get_predictors(col)\n            weights = self.get_predictor_weights(col)\n\n            if predictors:\n                # Score based on both count and strength of correlations\n                # Higher score = better predictors = impute later (use as predictor for others first)\n                avg_weight = sum(weights.values()) / len(weights) if weights else 0\n                score = len(predictors) * (1 + avg_weight)  # Weighted by average correlation strength\n            else:\n                score = 0\n\n            scores[col] = score\n\n        # Sort by score (ascending) - impute columns with weaker predictors first\n        # This allows later imputations to benefit from strongly predicted columns\n        ordered = sorted(scores.items(), key=lambda x: x[1])\n\n        return [col for col, _ in ordered]\n\n    def visualize_correlations(self, figsize: Tuple[int, int] = (12, 10)):\n        \"\"\"\n        Create a heatmap visualization of the correlation matrix.\n\n        Parameters:\n        -----------\n        figsize : Tuple[int, int]\n            Figure size for the plot\n        \"\"\"\n        plt.figure(figsize=figsize)\n        sns.heatmap(\n            self.correlation_matrix,\n            annot=True,\n            cmap='coolwarm',\n            center=0,\n            vmin=-1,\n            vmax=1,\n            fmt='.2f'\n        )\n        plt.title('Feature Correlation Matrix')\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmm_imputer_header"
   },
   "source": [
    "## 2. Predictive Mean Matching (PMM) Imputer Module\n",
    "\n",
    "Implements PMM algorithm for semi-parametric imputation that preserves data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmm_imputer"
   },
   "outputs": [],
   "source": "class PMMImputer:\n    \"\"\"\n    Predictive Mean Matching (PMM) imputer.\n\n    PMM is a semi-parametric imputation method that:\n    1. Fits a prediction model on observed data\n    2. Predicts values for missing data\n    3. Finds observed values with similar predictions\n    4. Randomly selects from these \"donor\" values\n\n    This preserves the distribution of the original data better than\n    simple regression imputation.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_neighbors: int = 5,\n        model_type: str = 'linear',\n        random_state: Optional[int] = None\n    ):\n        \"\"\"\n        Initialize the PMM imputer.\n\n        Parameters:\n        -----------\n        n_neighbors : int, default=5\n            Number of nearest neighbors to consider for donor pool\n        model_type : str, default='linear'\n            Type of prediction model: 'linear', 'bayesian', or 'rf' (random forest)\n        random_state : int, optional\n            Random state for reproducibility\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.model_type = model_type\n        self.random_state = random_state\n        self.model = None\n        self.scaler = StandardScaler()\n\n        # Initialize the prediction model\n        if model_type == 'linear':\n            self.model = LinearRegression()\n        elif model_type == 'bayesian':\n            self.model = BayesianRidge()\n        elif model_type == 'rf':\n            self.model = RandomForestRegressor(\n                n_estimators=100,\n                random_state=random_state\n            )\n        else:\n            raise ValueError(f\"Unknown model_type: {model_type}\")\n\n        self.rng = np.random.RandomState(random_state)\n\n    def fit_transform(\n        self,\n        data: pd.DataFrame,\n        target_column: str,\n        predictor_columns: List[str],\n        predictor_weights: Optional[Dict[str, float]] = None\n    ) -> np.ndarray:\n        \"\"\"\n        Impute missing values in the target column using enhanced PMM.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            The dataset\n        target_column : str\n            Column to impute\n        predictor_columns : List[str]\n            Columns to use as predictors\n        predictor_weights : Dict[str, float], optional\n            Weights for each predictor (correlation strengths)\n\n        Returns:\n        --------\n        imputed_values : np.ndarray\n            The imputed column (complete, no missing values)\n        \"\"\"\n        # Separate observed and missing data\n        target = data[target_column]\n        predictors = data[predictor_columns]\n\n        # Handle case where predictors might have missing values\n        # For now, we'll only use complete cases in predictors\n        complete_mask = ~predictors.isnull().any(axis=1)\n        observed_mask = ~target.isnull() & complete_mask\n        missing_mask = target.isnull() & complete_mask\n\n        if missing_mask.sum() == 0:\n            # No missing values to impute\n            return target.values\n\n        if observed_mask.sum() < self.n_neighbors:\n            # Not enough observed data for PMM, fall back to mean imputation\n            mean_value = target[observed_mask].mean()\n            result = target.copy()\n            result[missing_mask] = mean_value\n            return result.values\n\n        # Get observed and missing predictor matrices\n        X_observed = predictors[observed_mask]\n        X_missing = predictors[missing_mask]\n        y_observed = target[observed_mask].values\n\n        # Create weight vector for predictors\n        if predictor_weights:\n            weight_vector = np.array([predictor_weights.get(col, 1.0) for col in predictor_columns])\n            # Normalize to reasonable scale\n            weight_vector = weight_vector / weight_vector.sum() * len(weight_vector)\n        else:\n            weight_vector = np.ones(len(predictor_columns))\n\n        # Scale the predictors with weights\n        X_observed_scaled = self.scaler.fit_transform(X_observed)\n        X_missing_scaled = self.scaler.transform(X_missing)\n\n        # Apply weights to scaled predictors\n        X_observed_weighted = X_observed_scaled * weight_vector\n        X_missing_weighted = X_missing_scaled * weight_vector\n\n        # Fit the prediction model\n        self.model.fit(X_observed_weighted, y_observed)\n\n        # Predict for both observed and missing\n        # Add small stochastic noise to predictions for uncertainty\n        y_observed_pred = self.model.predict(X_observed_weighted)\n        y_missing_pred = self.model.predict(X_missing_weighted)\n\n        # Add stochastic component based on residual standard deviation\n        residuals = y_observed - y_observed_pred\n        residual_std = np.std(residuals)\n\n        # Add noise to missing predictions (stochastic regression component)\n        if residual_std > 0:\n            noise = self.rng.normal(0, residual_std * 0.5, len(y_missing_pred))\n            y_missing_pred_noisy = y_missing_pred + noise\n        else:\n            y_missing_pred_noisy = y_missing_pred\n\n        # For each missing value, find donors and select one\n        imputed_values = np.zeros(missing_mask.sum())\n\n        for i, (pred_value, x_missing) in enumerate(zip(y_missing_pred_noisy, X_missing_weighted)):\n            # Compute combined distance: prediction space + predictor space\n            # Distance in prediction space (primary)\n            pred_distances = np.abs(y_observed_pred - pred_value)\n\n            # Distance in predictor space (secondary, for tie-breaking)\n            predictor_distances = np.sqrt(np.sum((X_observed_weighted - x_missing) ** 2, axis=1))\n\n            # Normalize both distances\n            pred_dist_norm = pred_distances / (np.std(pred_distances) + 1e-10)\n            predictor_dist_norm = predictor_distances / (np.std(predictor_distances) + 1e-10)\n\n            # Combined distance: 70% prediction, 30% predictor space\n            combined_distances = 0.7 * pred_dist_norm + 0.3 * predictor_dist_norm\n\n            # Find the k nearest neighbors\n            n_donors = min(self.n_neighbors, len(combined_distances))\n            nearest_indices = np.argpartition(combined_distances, n_donors - 1)[:n_donors]\n\n            # Weighted random selection from donors\n            # Donors closer in distance have higher probability\n            donor_distances = combined_distances[nearest_indices]\n            # Convert distances to weights (inverse distance)\n            donor_weights = 1.0 / (donor_distances + 0.01)  # Add small constant to avoid division by zero\n            donor_weights = donor_weights / donor_weights.sum()\n\n            # Select donor with weighted probability\n            donor_idx = self.rng.choice(nearest_indices, p=donor_weights)\n            imputed_values[i] = y_observed[donor_idx]\n\n        # Create result array\n        result = target.copy()\n        result[missing_mask] = imputed_values\n\n        return result.values\n\n    def impute_column(\n        self,\n        data: pd.DataFrame,\n        target_column: str,\n        predictor_columns: Optional[List[str]] = None,\n        predictor_weights: Optional[Dict[str, float]] = None\n    ) -> pd.Series:\n        \"\"\"\n        Convenience method to impute a single column and return as Series.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            The dataset\n        target_column : str\n            Column to impute\n        predictor_columns : List[str], optional\n            Columns to use as predictors. If None, uses all other columns.\n        predictor_weights : Dict[str, float], optional\n            Weights for each predictor (correlation strengths)\n\n        Returns:\n        --------\n        imputed_column : pd.Series\n            The imputed column\n        \"\"\"\n        if predictor_columns is None:\n            predictor_columns = [col for col in data.columns if col != target_column]\n\n        imputed_values = self.fit_transform(data, target_column, predictor_columns, predictor_weights)\n        return pd.Series(imputed_values, index=data.index, name=target_column)\n\n\nclass AdaptivePMMImputer(PMMImputer):\n    \"\"\"\n    Adaptive PMM imputer that adjusts n_neighbors based on data availability.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_neighbors: int = 5,\n        min_neighbors: int = 3,\n        model_type: str = 'linear',\n        random_state: Optional[int] = None\n    ):\n        \"\"\"\n        Initialize adaptive PMM imputer.\n\n        Parameters:\n        -----------\n        n_neighbors : int, default=5\n            Target number of nearest neighbors\n        min_neighbors : int, default=3\n            Minimum number of neighbors to use\n        model_type : str, default='linear'\n            Type of prediction model\n        random_state : int, optional\n            Random state for reproducibility\n        \"\"\"\n        super().__init__(n_neighbors, model_type, random_state)\n        self.min_neighbors = min_neighbors\n\n    def fit_transform(\n        self,\n        data: pd.DataFrame,\n        target_column: str,\n        predictor_columns: List[str],\n        predictor_weights: Optional[Dict[str, float]] = None\n    ) -> np.ndarray:\n        \"\"\"\n        Impute with adaptive neighbor selection.\n        \"\"\"\n        target = data[target_column]\n        predictors = data[predictor_columns]\n\n        complete_mask = ~predictors.isnull().any(axis=1)\n        observed_mask = ~target.isnull() & complete_mask\n        n_observed = observed_mask.sum()\n\n        # Adapt n_neighbors based on available data\n        if n_observed < self.min_neighbors:\n            # Fall back to mean imputation\n            mean_value = target[observed_mask].mean()\n            result = target.copy()\n            result[target.isnull()] = mean_value\n            return result.values\n\n        # Adjust n_neighbors to available data\n        original_n = self.n_neighbors\n        self.n_neighbors = min(self.n_neighbors, max(self.min_neighbors, n_observed // 3))\n\n        # Call parent fit_transform\n        result = super().fit_transform(data, target_column, predictor_columns, predictor_weights)\n\n        # Restore original n_neighbors\n        self.n_neighbors = original_n\n\n        return result"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hybrid_imputer_header"
   },
   "source": [
    "## 3. Hybrid MICE Imputer Module\n",
    "\n",
    "Combines Correlation Analysis + PMM + MICE for advanced missing data imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hybrid_imputer"
   },
   "outputs": [],
   "source": "class HybridMICEImputer:\n    \"\"\"\n    Hybrid imputation model combining:\n    - Correlation Analysis: To identify optimal predictor sets\n    - PMM (Predictive Mean Matching): For distribution-preserving imputation\n    - MICE (Multivariate Imputation by Chained Equations): For iterative refinement\n\n    This hybrid approach:\n    1. Uses correlation analysis to determine which features best predict each missing variable\n    2. Employs PMM to impute values while preserving data distribution\n    3. Iterates using MICE framework to refine imputations based on newly imputed values\n    \"\"\"\n\n    def __init__(\n        self,\n        n_iterations: int = 15,\n        n_neighbors: int = 10,\n        correlation_threshold: float = 0.25,\n        max_predictors: int = 15,\n        pmm_model_type: str = 'bayesian',\n        convergence_threshold: float = 0.001,\n        random_state: Optional[int] = None,\n        verbose: bool = False,\n        exclude_columns: Optional[List[str]] = None,\n        use_mixed_correlations: bool = True\n    ):\n        \"\"\"\n        Initialize the Hybrid MICE imputer with enhanced correlation and PMM.\n\n        Parameters:\n        -----------\n        n_iterations : int, default=15\n            Maximum number of MICE iterations (increased for better convergence)\n        n_neighbors : int, default=10\n            Number of neighbors for PMM (increased for better donor pool)\n        correlation_threshold : float, default=0.25\n            Minimum correlation to consider for predictor selection (lowered for more predictors)\n        max_predictors : int, default=15\n            Maximum number of predictors to use per variable (increased for better modeling)\n        pmm_model_type : str, default='bayesian'\n            Model type for PMM: 'linear', 'bayesian', or 'rf' (bayesian for uncertainty)\n        convergence_threshold : float, default=0.001\n            Threshold for convergence detection\n        random_state : int, optional\n            Random state for reproducibility\n        verbose : bool, default=False\n            Whether to print progress information\n        exclude_columns : List[str], optional\n            Columns to exclude from imputation (e.g., ID columns like 'ptid')\n        use_mixed_correlations : bool, default=True\n            Use mixed correlation methods (Pearson, Spearman, Kendall) for robust estimation\n        \"\"\"\n        self.n_iterations = n_iterations\n        self.n_neighbors = n_neighbors\n        self.correlation_threshold = correlation_threshold\n        self.max_predictors = max_predictors\n        self.pmm_model_type = pmm_model_type\n        self.convergence_threshold = convergence_threshold\n        self.random_state = random_state\n        self.verbose = verbose\n        self.exclude_columns = exclude_columns or []\n        self.use_mixed_correlations = use_mixed_correlations\n\n        # Components\n        self.correlation_analyzer = CorrelationAnalyzer(\n            correlation_threshold=correlation_threshold,\n            use_mixed_correlations=use_mixed_correlations\n        )\n        self.imputer = AdaptivePMMImputer(\n            n_neighbors=n_neighbors,\n            model_type=pmm_model_type,\n            random_state=random_state\n        )\n\n        # State\n        self.missing_indicators = None\n        self.columns_with_missing = []\n        self.numeric_columns_with_missing = []\n        self.imputation_order = []\n        self.convergence_history = []\n\n    def _identify_missing_data(self, data: pd.DataFrame) -> None:\n        \"\"\"Identify columns with missing data and create missing indicators.\"\"\"\n        self.missing_indicators = data.isnull()\n        self.columns_with_missing = [\n            col for col in data.columns if self.missing_indicators[col].any()\n        ]\n\n        # Filter to only numeric columns for PMM imputation\n        # Exclude non-numeric columns and any user-specified exclusions\n        self.numeric_columns_with_missing = [\n            col for col in self.columns_with_missing\n            if col not in self.exclude_columns\n            and data[col].dtype in [np.float64, np.float32, np.int64, np.int32, np.float16, np.int16, np.int8]\n        ]\n\n        if self.verbose:\n            print(f\"Columns with missing data: {len(self.columns_with_missing)}\")\n            print(f\"Numeric columns to impute with PMM: {len(self.numeric_columns_with_missing)}\")\n            if len(self.columns_with_missing) > len(self.numeric_columns_with_missing):\n                excluded = set(self.columns_with_missing) - set(self.numeric_columns_with_missing)\n                print(f\"Excluded columns (non-numeric or specified): {list(excluded)}\")\n            print()\n            for col in self.columns_with_missing:\n                n_missing = self.missing_indicators[col].sum()\n                pct_missing = 100 * n_missing / len(data)\n                col_type = \"numeric (PMM)\" if col in self.numeric_columns_with_missing else \"excluded\"\n                print(f\"  {col}: {n_missing} ({pct_missing:.2f}%) - {col_type}\")\n\n    def _initialize_imputation(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Initialize missing values with simple mean imputation.\n        This provides a starting point for the MICE iterations.\n        \"\"\"\n        imputed = data.copy()\n\n        for col in self.columns_with_missing:\n            if imputed[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n                # Numerical: use mean\n                mean_val = imputed[col].mean()\n                imputed.loc[:, col] = imputed[col].fillna(mean_val)\n            else:\n                # Categorical: use mode\n                mode_val = imputed[col].mode()[0] if len(imputed[col].mode()) > 0 else imputed[col].iloc[0]\n                imputed.loc[:, col] = imputed[col].fillna(mode_val)\n\n        return imputed\n\n    def _compute_convergence_metric(\n        self,\n        data_current: pd.DataFrame,\n        data_previous: pd.DataFrame\n    ) -> float:\n        \"\"\"\n        Compute convergence metric based on change in imputed values.\n        Only considers numeric columns that are being imputed.\n        Uses robust standardization and tracks both mean and max changes.\n        \"\"\"\n        if data_previous is None:\n            return float('inf')\n\n        changes = []\n\n        # Only compute convergence for numeric columns being imputed\n        for col in self.numeric_columns_with_missing:\n            missing_mask = self.missing_indicators[col]\n            if missing_mask.any():\n                current_vals = data_current.loc[missing_mask, col]\n                previous_vals = data_previous.loc[missing_mask, col]\n\n                # Compute absolute differences\n                abs_diff = np.abs(current_vals - previous_vals)\n\n                # Normalize by robust standard deviation (using MAD - median absolute deviation)\n                col_values = data_current[col].values\n                median = np.median(col_values)\n                mad = np.median(np.abs(col_values - median))\n\n                if mad > 0:\n                    # Use MAD for robust scaling\n                    normalized_change = abs_diff / (1.4826 * mad)  # 1.4826 * MAD approximates std\n                else:\n                    # Fallback to standard deviation\n                    std = data_current[col].std()\n                    if std > 0:\n                        normalized_change = abs_diff / std\n                    else:\n                        # If no variation, use absolute change\n                        normalized_change = abs_diff\n\n                # Use mean change for this column\n                changes.append(np.mean(normalized_change))\n\n        if len(changes) == 0:\n            return 0\n\n        # Return average change across all columns\n        # Also consider max change to ensure all columns have converged\n        mean_change = np.mean(changes)\n        max_change = np.max(changes)\n\n        # Weighted combination: prioritize mean but consider max\n        return 0.7 * mean_change + 0.3 * max_change\n\n    def fit_transform(\n        self,\n        data: pd.DataFrame,\n        columns_to_impute: Optional[List[str]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Impute missing values using the hybrid Correlation + PMM + MICE approach.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            Dataset with missing values\n        columns_to_impute : List[str], optional\n            Specific columns to impute. If None, imputes all columns with missing values.\n\n        Returns:\n        --------\n        imputed_data : pd.DataFrame\n            Dataset with imputed values\n        \"\"\"\n        if data.isnull().sum().sum() == 0:\n            if self.verbose:\n                print(\"No missing data found. Returning original data.\")\n            return data.copy()\n\n        # Identify missing data\n        self._identify_missing_data(data)\n\n        # Filter to requested columns\n        if columns_to_impute is not None:\n            self.columns_with_missing = [\n                col for col in self.columns_with_missing if col in columns_to_impute\n            ]\n            self.numeric_columns_with_missing = [\n                col for col in self.numeric_columns_with_missing if col in columns_to_impute\n            ]\n\n        if not self.numeric_columns_with_missing:\n            if self.verbose:\n                print(\"No numeric columns to impute with PMM. Returning data with simple imputation for non-numeric columns.\")\n            # Still do simple imputation for non-numeric columns\n            return self._initialize_imputation(data)\n\n        # Initialize with simple imputation\n        imputed = self._initialize_imputation(data)\n\n        # Fit correlation analyzer on complete cases first, but only on numeric columns\n        numeric_cols = [col for col in data.columns\n                       if data[col].dtype in [np.float64, np.float32, np.int64, np.int32, np.float16, np.int16, np.int8]]\n        complete_data = data[numeric_cols].dropna()\n        if len(complete_data) > 0:\n            self.correlation_analyzer.fit(complete_data)\n        else:\n            # If no complete cases, use initialized numeric data\n            self.correlation_analyzer.fit(imputed[numeric_cols])\n\n        # Determine imputation order based on correlations (only for numeric columns)\n        self.imputation_order = self.correlation_analyzer.get_imputation_order(\n            self.numeric_columns_with_missing\n        )\n\n        if self.verbose:\n            print(f\"Imputation order: {self.imputation_order}\")\n            print(f\"Starting MICE iterations (max {self.n_iterations})...\")\n\n        # MICE iterations\n        previous_imputed = None\n        self.convergence_history = []\n\n        for iteration in range(self.n_iterations):\n            if self.verbose:\n                print(f\"\\nIteration {iteration + 1}/{self.n_iterations}\")\n\n            # Iterate through each column with missing values\n            for col in self.imputation_order:\n                # Get correlation-based predictors\n                predictors = self.correlation_analyzer.get_predictors(\n                    col,\n                    max_predictors=self.max_predictors\n                )\n\n                # If no predictors found, use all other columns\n                if not predictors:\n                    predictors = [c for c in imputed.columns if c != col]\n\n                # Filter out predictors that are all NaN or have too many NaNs\n                valid_predictors = []\n                for pred in predictors:\n                    if imputed[pred].isnull().sum() < len(imputed) * 0.5:\n                        valid_predictors.append(pred)\n\n                if not valid_predictors:\n                    if self.verbose:\n                        print(f\"  {col}: No valid predictors, skipping\")\n                    continue\n\n                # Apply PMM imputation\n                try:\n                    # Create temporary dataset with only needed columns\n                    temp_data = imputed[[col] + valid_predictors].copy()\n\n                    # CRITICAL FIX: Restore original missing values in the target column\n                    # so that PMM can actually see them and perform proper imputation\n                    missing_mask = self.missing_indicators[col]\n                    temp_data.loc[missing_mask, col] = np.nan\n\n                    # Get correlation weights for predictors\n                    predictor_weights = self.correlation_analyzer.get_predictor_weights(col)\n                    # Filter weights to only valid predictors\n                    filtered_weights = {k: v for k, v in predictor_weights.items() if k in valid_predictors}\n\n                    # Impute the column using PMM with correlation weights\n                    imputed_col = self.imputer.fit_transform(\n                        temp_data,\n                        col,\n                        valid_predictors,\n                        predictor_weights=filtered_weights\n                    )\n\n                    # Update only the originally missing values\n                    imputed.loc[missing_mask, col] = imputed_col[missing_mask]\n\n                    if self.verbose:\n                        n_weighted = len(filtered_weights)\n                        print(f\"  {col}: Imputed with {len(valid_predictors)} predictors ({n_weighted} weighted)\")\n\n                except Exception as e:\n                    if self.verbose:\n                        print(f\"  {col}: Error during imputation - {str(e)}\")\n                    continue\n\n            # Check convergence\n            convergence = self._compute_convergence_metric(imputed, previous_imputed)\n            self.convergence_history.append(convergence)\n\n            if self.verbose:\n                print(f\"Convergence metric: {convergence:.6f}\")\n\n            if convergence < self.convergence_threshold:\n                if self.verbose:\n                    print(f\"Converged at iteration {iteration + 1}\")\n                break\n\n            previous_imputed = imputed.copy()\n\n        if self.verbose:\n            print(\"\\nImputation complete!\")\n            print(f\"Total iterations: {len(self.convergence_history)}\")\n\n        return imputed\n\n    def fit(self, data: pd.DataFrame) -> 'HybridMICEImputer':\n        \"\"\"\n        Fit the imputer (mainly for correlation analysis).\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            Training data\n\n        Returns:\n        --------\n        self : HybridMICEImputer\n        \"\"\"\n        # Only fit on numeric columns\n        numeric_cols = [col for col in data.columns\n                       if data[col].dtype in [np.float64, np.float32, np.int64, np.int32, np.float16, np.int16, np.int8]]\n        complete_data = data[numeric_cols].dropna()\n        if len(complete_data) > 0:\n            self.correlation_analyzer.fit(complete_data)\n        else:\n            # If no complete cases, use all numeric data\n            self.correlation_analyzer.fit(data[numeric_cols])\n\n        return self\n\n    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform data using fitted imputer.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            Data to impute\n\n        Returns:\n        --------\n        imputed_data : pd.DataFrame\n        \"\"\"\n        return self.fit_transform(data)\n\n    def get_diagnostics(self) -> Dict[str, Union[List, pd.DataFrame]]:\n        \"\"\"\n        Get diagnostic information about the imputation process.\n\n        Returns:\n        --------\n        diagnostics : dict\n            Dictionary containing diagnostic information\n        \"\"\"\n        excluded_columns = list(set(self.columns_with_missing) - set(self.numeric_columns_with_missing))\n\n        diagnostics = {\n            'convergence_history': self.convergence_history,\n            'imputation_order': self.imputation_order,\n            'columns_with_missing': self.columns_with_missing,\n            'numeric_columns_imputed': self.numeric_columns_with_missing,\n            'excluded_columns': excluded_columns,\n            'correlation_matrix': self.correlation_analyzer.correlation_matrix,\n            'predictor_sets': self.correlation_analyzer.predictor_sets\n        }\n\n        return diagnostics\n\n    def plot_convergence(self):\n        \"\"\"\n        Plot the convergence history.\n        \"\"\"\n        if not self.convergence_history:\n            print(\"No convergence history available. Run fit_transform first.\")\n            return\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(range(1, len(self.convergence_history) + 1), self.convergence_history, marker='o')\n        plt.axhline(y=self.convergence_threshold, color='r', linestyle='--', label='Convergence threshold')\n        plt.xlabel('Iteration')\n        plt.ylabel('Convergence Metric')\n        plt.title('MICE Convergence History')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n\n    @staticmethod\n    def load_data(\n        file_path: str,\n        sheet_name: Optional[Union[str, int]] = 0,\n        **kwargs\n    ) -> pd.DataFrame:\n        \"\"\"\n        Load data from CSV or Excel (.xlsx) file.\n\n        Parameters:\n        -----------\n        file_path : str\n            Path to the file (supports .csv, .xlsx, .xls)\n        sheet_name : str or int, default=0\n            Sheet name or index for Excel files (ignored for CSV)\n        **kwargs : dict\n            Additional arguments passed to pd.read_csv() or pd.read_excel()\n\n        Returns:\n        --------\n        data : pd.DataFrame\n            Loaded data\n\n        Examples:\n        ---------\n        >>> # Load CSV file\n        >>> data = HybridMICEImputer.load_data('data.csv')\n        >>>\n        >>> # Load Excel file (first sheet)\n        >>> data = HybridMICEImputer.load_data('data.xlsx')\n        >>>\n        >>> # Load specific sheet from Excel\n        >>> data = HybridMICEImputer.load_data('data.xlsx', sheet_name='Sheet2')\n        >>> data = HybridMICEImputer.load_data('data.xlsx', sheet_name=1)\n        \"\"\"\n        file_path = str(file_path)\n\n        if file_path.endswith('.csv'):\n            return pd.read_csv(file_path, **kwargs)\n        elif file_path.endswith(('.xlsx', '.xls')):\n            return pd.read_excel(file_path, sheet_name=sheet_name, **kwargs)\n        else:\n            # Try to infer format\n            try:\n                return pd.read_excel(file_path, sheet_name=sheet_name, **kwargs)\n            except:\n                return pd.read_csv(file_path, **kwargs)\n\n    @staticmethod\n    def save_data(\n        data: pd.DataFrame,\n        file_path: str,\n        sheet_name: str = 'Sheet1',\n        index: bool = False,\n        **kwargs\n    ) -> None:\n        \"\"\"\n        Save data to CSV or Excel (.xlsx) file.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            Data to save\n        file_path : str\n            Path to save the file (supports .csv, .xlsx)\n        sheet_name : str, default='Sheet1'\n            Sheet name for Excel files (ignored for CSV)\n        index : bool, default=False\n            Whether to include the index in the saved file\n        **kwargs : dict\n            Additional arguments passed to pd.to_csv() or pd.to_excel()\n\n        Examples:\n        ---------\n        >>> # Save to CSV\n        >>> HybridMICEImputer.save_data(imputed_data, 'output.csv')\n        >>>\n        >>> # Save to Excel\n        >>> HybridMICEImputer.save_data(imputed_data, 'output.xlsx')\n        >>>\n        >>> # Save to Excel with custom sheet name\n        >>> HybridMICEImputer.save_data(imputed_data, 'output.xlsx', sheet_name='Imputed Data')\n        \"\"\"\n        file_path = str(file_path)\n\n        if file_path.endswith('.csv'):\n            data.to_csv(file_path, index=index, **kwargs)\n        elif file_path.endswith('.xlsx'):\n            data.to_excel(file_path, sheet_name=sheet_name, index=index, **kwargs)\n        else:\n            # Default to CSV if extension not recognized\n            data.to_csv(file_path, index=index, **kwargs)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper_functions_header"
   },
   "source": [
    "## 4. Helper Functions\n",
    "\n",
    "Function to create sample data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper_functions"
   },
   "outputs": [],
   "source": [
    "def create_sample_data_with_missing(n_samples=1000, missing_rate=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a sample dataset with missing values for demonstration.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    missing_rate : float\n",
    "        Proportion of values to set as missing\n",
    "    random_state : int\n",
    "        Random seed\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data : pd.DataFrame\n",
    "        Dataset with missing values\n",
    "    data_complete : pd.DataFrame\n",
    "        Original complete dataset (for comparison)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Generate correlated features\n",
    "    # Feature 1: base random variable\n",
    "    x1 = np.random.randn(n_samples)\n",
    "\n",
    "    # Feature 2: strongly correlated with x1\n",
    "    x2 = 0.8 * x1 + 0.2 * np.random.randn(n_samples)\n",
    "\n",
    "    # Feature 3: moderately correlated with x1 and x2\n",
    "    x3 = 0.5 * x1 + 0.3 * x2 + 0.4 * np.random.randn(n_samples)\n",
    "\n",
    "    # Feature 4: weakly correlated\n",
    "    x4 = 0.2 * x1 + 0.8 * np.random.randn(n_samples)\n",
    "\n",
    "    # Feature 5: independent\n",
    "    x5 = np.random.randn(n_samples)\n",
    "\n",
    "    # Create DataFrame\n",
    "    data_complete = pd.DataFrame({\n",
    "        'feature1': x1,\n",
    "        'feature2': x2,\n",
    "        'feature3': x3,\n",
    "        'feature4': x4,\n",
    "        'feature5': x5\n",
    "    })\n",
    "\n",
    "    # Add a target variable\n",
    "    data_complete['target'] = (\n",
    "        2 * x1 + 1.5 * x2 - 0.5 * x3 + np.random.randn(n_samples) * 0.5\n",
    "    )\n",
    "\n",
    "    # Create missing values\n",
    "    data_with_missing = data_complete.copy()\n",
    "\n",
    "    for col in data_with_missing.columns:\n",
    "        # Randomly select indices to set as missing\n",
    "        n_missing = int(n_samples * missing_rate)\n",
    "        missing_indices = np.random.choice(n_samples, n_missing, replace=False)\n",
    "        data_with_missing.loc[missing_indices, col] = np.nan\n",
    "\n",
    "    return data_with_missing, data_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examples_header"
   },
   "source": [
    "---\n",
    "\n",
    "# Examples and Usage\n",
    "\n",
    "Let's explore different use cases of the Hybrid MICE Imputer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example1_header"
   },
   "source": [
    "## Example 1: Basic Usage\n",
    "\n",
    "Demonstrate basic imputation with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example1"
   },
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data_missing, data_complete = create_sample_data_with_missing(\n",
    "    n_samples=500,\n",
    "    missing_rate=0.15\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Example 1: Basic Usage\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nOriginal data with missing values:\")\n",
    "print(data_missing.head(10))\n",
    "print(\"\\nMissing value statistics:\")\n",
    "print(data_missing.isnull().sum())\n",
    "\n",
    "# Initialize and fit the hybrid imputer\n",
    "imputer = HybridMICEImputer(\n",
    "    n_iterations=10,\n",
    "    n_neighbors=5,\n",
    "    correlation_threshold=0.3,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Impute missing values\n",
    "data_imputed = imputer.fit_transform(data_missing)\n",
    "\n",
    "print(\"\\n\\nImputed data:\")\n",
    "print(data_imputed.head(10))\n",
    "\n",
    "# Calculate imputation accuracy (RMSE)\n",
    "print(\"\\n\\nImputation Quality Assessment:\")\n",
    "for col in data_missing.columns:\n",
    "    missing_mask = data_missing[col].isnull()\n",
    "    if missing_mask.any():\n",
    "        true_values = data_complete.loc[missing_mask, col]\n",
    "        imputed_values = data_imputed.loc[missing_mask, col]\n",
    "        rmse = np.sqrt(np.mean((true_values - imputed_values) ** 2))\n",
    "        print(f\"{col}: RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example2_header"
   },
   "source": [
    "## Example 2: Comparing Different Model Types\n",
    "\n",
    "Compare performance of different PMM model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example2"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 2: Comparing Model Types\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create data with more missing values\n",
    "data_missing, data_complete = create_sample_data_with_missing(\n",
    "    n_samples=300,\n",
    "    missing_rate=0.30\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(data_missing)} samples\")\n",
    "print(f\"Missing rate: ~30%\")\n",
    "\n",
    "# Try different model configurations\n",
    "configs = [\n",
    "    {'name': 'Linear PMM', 'pmm_model_type': 'linear'},\n",
    "    {'name': 'Bayesian PMM', 'pmm_model_type': 'bayesian'},\n",
    "    {'name': 'Random Forest PMM', 'pmm_model_type': 'rf'}\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\n\\nTesting: {config['name']}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    imputer = HybridMICEImputer(\n",
    "        n_iterations=15,\n",
    "        n_neighbors=7,\n",
    "        pmm_model_type=config['pmm_model_type'],\n",
    "        correlation_threshold=0.25,\n",
    "        verbose=False,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    data_imputed = imputer.fit_transform(data_missing)\n",
    "\n",
    "    # Calculate overall RMSE\n",
    "    total_rmse = 0\n",
    "    n_cols = 0\n",
    "\n",
    "    for col in data_missing.columns:\n",
    "        missing_mask = data_missing[col].isnull()\n",
    "        if missing_mask.any():\n",
    "            true_values = data_complete.loc[missing_mask, col]\n",
    "            imputed_values = data_imputed.loc[missing_mask, col]\n",
    "            rmse = np.sqrt(np.mean((true_values - imputed_values) ** 2))\n",
    "            total_rmse += rmse\n",
    "            n_cols += 1\n",
    "\n",
    "    avg_rmse = total_rmse / n_cols if n_cols > 0 else 0\n",
    "    results[config['name']] = avg_rmse\n",
    "\n",
    "    print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "    print(f\"Iterations to convergence: {len(imputer.convergence_history)}\")\n",
    "\n",
    "print(\"\\n\\nComparison Summary:\")\n",
    "print(\"-\"*40)\n",
    "for name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
    "    print(f\"{name}: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example3_header"
   },
   "source": [
    "## Example 3: Diagnostics and Visualization\n",
    "\n",
    "Explore diagnostic features and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example3"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 3: Diagnostics and Visualization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sample data\n",
    "data_missing, _ = create_sample_data_with_missing(\n",
    "    n_samples=400,\n",
    "    missing_rate=0.20\n",
    ")\n",
    "\n",
    "# Initialize imputer\n",
    "imputer = HybridMICEImputer(\n",
    "    n_iterations=20,\n",
    "    verbose=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Impute\n",
    "data_imputed = imputer.fit_transform(data_missing)\n",
    "\n",
    "# Get diagnostics\n",
    "diagnostics = imputer.get_diagnostics()\n",
    "\n",
    "print(\"\\nImputation Order (based on correlations):\")\n",
    "for i, col in enumerate(diagnostics['imputation_order'], 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "print(\"\\n\\nPredictor Sets (correlation-based):\")\n",
    "for col, predictors in diagnostics['predictor_sets'].items():\n",
    "    if col in diagnostics['columns_with_missing']:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Top predictors: {predictors[:3]}\")\n",
    "\n",
    "print(\"\\n\\nCorrelation Matrix:\")\n",
    "print(diagnostics['correlation_matrix'])\n",
    "\n",
    "print(\"\\n\\nConvergence History:\")\n",
    "for i, conv in enumerate(diagnostics['convergence_history'], 1):\n",
    "    print(f\"Iteration {i}: {conv:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plot_convergence_header"
   },
   "source": [
    "### Visualize Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_convergence"
   },
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "imputer.plot_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plot_correlations_header"
   },
   "source": [
    "### Visualize Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_correlations"
   },
   "outputs": [],
   "source": [
    "# Visualize correlation matrix\n",
    "imputer.correlation_analyzer.visualize_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example4_header"
   },
   "source": [
    "## Example 4: Partial Imputation (Specific Columns)\n",
    "\n",
    "Demonstrate imputing only specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example4"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 4: Partial Imputation (Specific Columns)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sample data\n",
    "data_missing, data_complete = create_sample_data_with_missing(\n",
    "    n_samples=500,\n",
    "    missing_rate=0.15\n",
    ")\n",
    "\n",
    "print(\"\\nImputing only 'feature1' and 'target' columns...\")\n",
    "\n",
    "# Initialize imputer\n",
    "imputer = HybridMICEImputer(\n",
    "    n_iterations=10,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Impute only specific columns\n",
    "data_imputed = imputer.fit_transform(\n",
    "    data_missing,\n",
    "    columns_to_impute=['feature1', 'target']\n",
    ")\n",
    "\n",
    "# Check which columns were imputed\n",
    "print(\"\\n\\nMissing values after imputation:\")\n",
    "print(data_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_data_header"
   },
   "source": [
    "## Example 5: Using Your Own Data\n",
    "\n",
    "Template for using the imputer with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_data"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 5: Using Your Own Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Uncomment and modify this section to use your own data:\n",
    "\n",
    "# # Load your data\n",
    "# your_data = pd.read_csv('your_file.csv')\n",
    "#\n",
    "# # Or upload a file in Google Colab:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# your_data = pd.read_csv(list(uploaded.keys())[0])\n",
    "#\n",
    "# # Initialize imputer with custom parameters\n",
    "# imputer = HybridMICEImputer(\n",
    "#     n_iterations=15,\n",
    "#     n_neighbors=5,\n",
    "#     correlation_threshold=0.3,\n",
    "#     pmm_model_type='linear',  # Options: 'linear', 'bayesian', 'rf'\n",
    "#     verbose=True,\n",
    "#     random_state=42\n",
    "# )\n",
    "#\n",
    "# # Impute missing values\n",
    "# your_data_imputed = imputer.fit_transform(your_data)\n",
    "#\n",
    "# # View results\n",
    "# print(\"\\nOriginal data:\")\n",
    "# print(your_data.head())\n",
    "# print(\"\\nMissing values:\", your_data.isnull().sum().sum())\n",
    "#\n",
    "# print(\"\\nImputed data:\")\n",
    "# print(your_data_imputed.head())\n",
    "# print(\"\\nMissing values:\", your_data_imputed.isnull().sum().sum())\n",
    "#\n",
    "# # Plot convergence\n",
    "# imputer.plot_convergence()\n",
    "#\n",
    "# # Download the imputed data\n",
    "# your_data_imputed.to_csv('imputed_data.csv', index=False)\n",
    "# files.download('imputed_data.csv')\n",
    "\n",
    "print(\"\\nUncomment and modify the code above to use your own data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Example 6: Handling Non-Numeric and ID Columns\n\nDemonstrates automatic exclusion of non-numeric columns and manual exclusion of ID columns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"Example 6: Handling Non-Numeric and ID Columns\")\nprint(\"=\"*70)\n\n# Create sample data with mixed types (like a real dataset)\nnp.random.seed(42)\nn_samples = 200\n\n# Create numeric features\nfeature1 = np.random.randn(n_samples)\nfeature2 = 0.7 * feature1 + 0.3 * np.random.randn(n_samples)\nage = np.random.randint(20, 80, n_samples).astype(float)\nscore = 50 + 10 * feature1 + np.random.randn(n_samples) * 5\n\n# Create non-numeric columns\nptid = [f\"PT{i:04d}\" for i in range(n_samples)]\ndiagnosis = np.random.choice(['Control', 'AD', 'MCI'], n_samples)\ngender = np.random.choice(['M', 'F'], n_samples)\n\n# Create DataFrame with mixed types\ndata_mixed = pd.DataFrame({\n    'ptid': ptid,\n    'diagnosis': diagnosis,\n    'gender': gender,\n    'age': age,\n    'feature1': feature1,\n    'feature2': feature2,\n    'score': score\n})\n\n# Introduce missing values in various columns\nmissing_rate = 0.20\nfor col in ['age', 'feature1', 'feature2', 'score', 'diagnosis']:\n    n_missing = int(n_samples * missing_rate)\n    missing_indices = np.random.choice(n_samples, n_missing, replace=False)\n    data_mixed.loc[missing_indices, col] = np.nan\n\nprint(\"\\nOriginal data with mixed types:\")\nprint(data_mixed.head(10))\nprint(\"\\nData types:\")\nprint(data_mixed.dtypes)\nprint(\"\\nMissing values:\")\nprint(data_mixed.isnull().sum())\n\n# Initialize imputer - it will automatically exclude non-numeric columns\n# and we can manually exclude 'ptid' as well\nimputer = HybridMICEImputer(\n    n_iterations=10,\n    n_neighbors=5,\n    correlation_threshold=0.3,\n    exclude_columns=['ptid'],  # Manually exclude patient ID\n    verbose=True,\n    random_state=42\n)\n\n# Impute missing values\ndata_imputed = imputer.fit_transform(data_mixed)\n\nprint(\"\\n\\nImputed data:\")\nprint(data_imputed.head(10))\nprint(\"\\nMissing values after imputation:\")\nprint(data_imputed.isnull().sum())\n\n# Get diagnostics to see what was excluded\ndiagnostics = imputer.get_diagnostics()\nprint(\"\\n\\nDiagnostic Information:\")\nprint(f\"Total columns with missing data: {len(diagnostics['columns_with_missing'])}\")\nprint(f\"Numeric columns imputed with PMM: {diagnostics['numeric_columns_imputed']}\")\nprint(f\"Excluded columns: {diagnostics['excluded_columns']}\")\nprint(\"\\nNote: Non-numeric columns (diagnosis, gender) and manually excluded columns (ptid)\")\nprint(\"      retain their original missing values or get simple mode imputation.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates the **Hybrid Correlation + PMM + MICE Imputer**, which combines:\n",
    "\n",
    "1. **Correlation Analysis** - Smart predictor selection based on feature correlations\n",
    "2. **Predictive Mean Matching** - Distribution-preserving imputation\n",
    "3. **MICE Framework** - Iterative refinement for improved accuracy\n",
    "\n",
    "### Key Features:\n",
    "- Multiple model types (Linear, Bayesian, Random Forest)\n",
    "- Automatic predictor selection\n",
    "- Convergence monitoring\n",
    "- Comprehensive diagnostics\n",
    "- Partial imputation support\n",
    "\n",
    "### When to Use:\n",
    "- Datasets with complex missing data patterns\n",
    "- When preserving data distribution is important\n",
    "- When features have strong correlations\n",
    "- For both research and production use cases\n",
    "\n",
    "---\n",
    "\n",
    "**Repository**: [https://github.com/CodeSakshamY/correlation-PMM-MICE](https://github.com/CodeSakshamY/correlation-PMM-MICE)\n",
    "\n",
    "**License**: MIT\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hybrid_Imputation_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}